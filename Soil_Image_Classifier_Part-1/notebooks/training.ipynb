{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil Classification Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "This section sets up the necessary libraries and configures GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T19:45:12.674958Z",
     "iopub.status.busy": "2025-05-23T19:45:12.674714Z",
     "iopub.status.idle": "2025-05-23T19:45:26.908728Z",
     "shell.execute_reply": "2025-05-23T19:45:26.908087Z",
     "shell.execute_reply.started": "2025-05-23T19:45:12.674934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This is the notebook used for training the model.\n",
    "\n",
    "# --- 1. Imports and Environment Setup ---\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from PIL import Image\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPU and mixed precision setup for faster training and efficiency ---\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    # Enable dynamic memory allocation for GPUs\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    # Enable XLA compilation for speed\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    # Enable mixed precision for faster computation\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(f\"GPU acceleration enabled: {len(physical_devices)} GPU(s) found\")\n",
    "    print(\"Mixed precision enabled\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Configuration ---\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Model and data configuration\n",
    "IMG_SIZE = 224         # Input image size (height, width)\n",
    "BATCH_SIZE = 64        # Images per batch\n",
    "EPOCHS = 12            # Number of training epochs\n",
    "NUM_CLASSES = 4        # Number of soil classes\n",
    "LEARNING_RATE = 0.001  # Initial learning rate\n",
    "\n",
    "# Paths to data directories and CSVs\n",
    "TRAIN_DIR = '/kaggle/input/soilcl/soil_classification-2025/train'\n",
    "TEST_DIR = '/kaggle/input/soilcl/soil_classification-2025/test'\n",
    "TRAIN_CSV = '/kaggle/input/soilcl/soil_classification-2025/train_labels.csv'\n",
    "TEST_CSV = '/kaggle/input/soilcl/soil_classification-2025/test_ids.csv'\n",
    "PROCESSED_TRAIN_DIR = '/kaggle/working/train'\n",
    "PROCESSED_TEST_DIR = '/kaggle/working/test'\n",
    "\n",
    "# Ensure processed directories exist\n",
    "os.makedirs(PROCESSED_TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_TEST_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "We convert all images to JPG and split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Image Conversion Utility ---\n",
    "def convert_to_jpg(source_dir, target_dir, file_mapping=None):\n",
    "    \"\"\"\n",
    "    Converts all images in source_dir to JPG format and saves them in target_dir.\n",
    "    Maintains a mapping from original filename to new filename.\n",
    "    \"\"\"\n",
    "    if file_mapping is None:\n",
    "        file_mapping = {}\n",
    "    for filename in os.listdir(source_dir):\n",
    "        source_path = os.path.join(source_dir, filename)\n",
    "        if not os.path.isfile(source_path):\n",
    "            continue\n",
    "        file_ext = os.path.splitext(filename)[1].lower()\n",
    "        if file_ext in ['.jpg', '.jpeg']:\n",
    "            # Already JPG, just copy\n",
    "            target_path = os.path.join(target_dir, filename)\n",
    "            shutil.copy2(source_path, target_path)\n",
    "            file_mapping[filename] = filename\n",
    "        else:\n",
    "            # Convert to JPG\n",
    "            try:\n",
    "                new_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "                target_path = os.path.join(target_dir, new_filename)\n",
    "                with Image.open(source_path) as img:\n",
    "                    img = img.convert('RGB')\n",
    "                    img.save(target_path, 'JPEG', quality=95)\n",
    "                file_mapping[filename] = new_filename\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {filename}: {e}\")\n",
    "                try:\n",
    "                    target_path = os.path.join(target_dir, filename)\n",
    "                    shutil.copy2(source_path, target_path)\n",
    "                    file_mapping[filename] = filename\n",
    "                except:\n",
    "                    print(f\"Could not process {filename}\")\n",
    "    return file_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Data Preparation and Augmentation ---\n",
    "def prepare_data_optimized():\n",
    "    \"\"\"\n",
    "    Prepares training, validation, and test data generators with augmentation.\n",
    "    Handles image conversion, dataset split, and class weighting for imbalance.\n",
    "    \"\"\"\n",
    "    print(\"Converting images to JPG format...\")\n",
    "    train_file_mapping = convert_to_jpg(TRAIN_DIR, PROCESSED_TRAIN_DIR)\n",
    "    test_file_mapping = convert_to_jpg(TEST_DIR, PROCESSED_TEST_DIR)\n",
    "    \n",
    "    # Read CSVs with image IDs and labels\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "    \n",
    "    # Map original image IDs to processed (JPG) filenames\n",
    "    train_df['processed_image_id'] = train_df['image_id'].map(lambda x: train_file_mapping.get(x, x))\n",
    "    test_df['processed_image_id'] = test_df['image_id'].map(lambda x: test_file_mapping.get(x, x))\n",
    "    \n",
    "    # Stratified split into training and validation sets (80/20)\n",
    "    train_data, val_data = train_test_split(\n",
    "        train_df, test_size=0.2, random_state=42, stratify=train_df['soil_type'])\n",
    "    \n",
    "    print(\"Training class distribution:\")\n",
    "    print(train_data['soil_type'].value_counts())\n",
    "    \n",
    "    # Data augmentation for training set\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        brightness_range=[0.7, 1.3],\n",
    "        fill_mode='nearest',\n",
    "        channel_shift_range=0.1\n",
    "    )\n",
    "    # Only rescaling for validation and test sets\n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Repeat training data to help with class imbalance (especially for minority classes)\n",
    "    repeated_train_data = train_data.loc[np.repeat(train_data.index.values, 3)]\n",
    "    \n",
    "    # Create data generators\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=repeated_train_data,\n",
    "        directory=PROCESSED_TRAIN_DIR,\n",
    "        x_col='processed_image_id',\n",
    "        y_col='soil_type',\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    valid_generator = valid_datagen.flow_from_dataframe(\n",
    "        dataframe=val_data,\n",
    "        directory=PROCESSED_TRAIN_DIR,\n",
    "        x_col='processed_image_id',\n",
    "        y_col='soil_type',\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        directory=PROCESSED_TEST_DIR,\n",
    "        x_col='processed_image_id',\n",
    "        y_col=None,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Compute class weights to help with imbalance (extra boost for \"Clay soil\" and \"Black Soil\")\n",
    "    class_weights = {}\n",
    "    total_samples = len(train_data)\n",
    "    soil_counts = train_data['soil_type'].value_counts()\n",
    "    for i, soil_type in enumerate(train_generator.class_indices):\n",
    "        count = soil_counts.get(soil_type, 0)\n",
    "        if count > 0:\n",
    "            class_weights[i] = (1 / count) * (total_samples / len(soil_counts))\n",
    "        # Boost weights for minority classes\n",
    "        if soil_type == 'Clay soil':\n",
    "            class_weights[i] *= 3.0\n",
    "        elif soil_type == 'Black Soil':\n",
    "            class_weights[i] *= 1.8\n",
    "    return train_generator, valid_generator, test_generator, train_data, val_data, test_df, class_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "We use a DenseNet121 backbone with additional layers for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Model Architecture ---\n",
    "def create_enhanced_densenet():\n",
    "    \"\"\"\n",
    "    Builds an enhanced DenseNet121 model with additional dense, dropout, and batch normalization layers.\n",
    "    Unfreezes last 30 layers for fine-tuning.\n",
    "    \"\"\"\n",
    "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    # Freeze all layers except the last 30 for fine-tuning\n",
    "    for layer in base_model.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # Add dense and dropout layers for better generalization\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    # Output layer for classification\n",
    "    predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    # Use Adam optimizer and macro F1 as metric\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.F1Score(average='macro')]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6. Evaluation Function ---\n",
    "def evaluate_enhanced_model(model, valid_generator, class_indices):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation set and prints macro and per-class F1 scores.\n",
    "    \"\"\"\n",
    "    valid_generator.reset()\n",
    "    y_pred_probs = model.predict(valid_generator, steps=int(np.ceil(valid_generator.samples/BATCH_SIZE)))\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = valid_generator.classes\n",
    "    # Calculate macro and individual F1 scores\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_individual = f1_score(y_true, y_pred, average=None)\n",
    "    idx_to_class = {v: k for k, v in class_indices.items()}\n",
    "    class_f1_scores = {idx_to_class[i]: score for i, score in enumerate(f1_individual)}\n",
    "    print(f\"Macro F1 Score: {f1_macro:.4f}\")\n",
    "    print(\"Individual F1 Scores:\")\n",
    "    for name, score in class_f1_scores.items():\n",
    "        print(f\" {name}: {score:.4f}\")\n",
    "    # Check if target F1 score is achieved\n",
    "    if f1_macro >= 0.95:\n",
    "        print(\"🎯 TARGET ACHIEVED: F1 Score ≥ 0.95!\")\n",
    "    else:\n",
    "        print(f\"📈 Progress: {f1_macro:.4f}/0.95 ({(f1_macro/0.95)*100:.1f}%)\")\n",
    "    return f1_macro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"target-heading\"></a>\n",
    "## 4. Training\n",
    "The model is trained with early stopping and learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T19:45:26.910891Z",
     "iopub.status.busy": "2025-05-23T19:45:26.910449Z",
     "iopub.status.idle": "2025-05-23T19:45:27.837390Z",
     "shell.execute_reply": "2025-05-23T19:45:27.836563Z",
     "shell.execute_reply.started": "2025-05-23T19:45:26.910872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 7. Main Training Pipeline ---\n",
    "def main_enhanced():\n",
    "    \"\"\"\n",
    "    Main training and evaluation workflow:\n",
    "    - Data preparation\n",
    "    - Model building\n",
    "    - Training with callbacks\n",
    "    - Evaluation\n",
    "    - Test prediction and submission file creation\n",
    "    \"\"\"\n",
    "    print(\"Starting enhanced soil classification for F1 ≥ 0.95...\")\n",
    "    # Prepare data generators and class weights\n",
    "    train_generator, valid_generator, test_generator, train_data, val_data, test_df, class_weights = prepare_data_optimized()\n",
    "    class_indices = train_generator.class_indices\n",
    "    print(f\"Class indices: {class_indices}\")\n",
    "    print(f\"Enhanced class weights: {class_weights}\")\n",
    "    # Build the model\n",
    "    print(\"Creating enhanced DenseNet121 model...\")\n",
    "    model = create_enhanced_densenet()\n",
    "    print(f\"Model parameters: {model.count_params():,}\")\n",
    "    # Define callbacks for early stopping and learning rate reduction\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00001)\n",
    "    ]\n",
    "    # Train the model\n",
    "    print(\"Training enhanced DenseNet121...\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=len(valid_generator),\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "    # Evaluate model performance\n",
    "    print(\"Evaluating enhanced model...\")\n",
    "    f1_score_result = evaluate_enhanced_model(model, valid_generator, class_indices)\n",
    "    # Generate predictions for test set\n",
    "    print(\"Generating test predictions...\")\n",
    "    test_generator.reset()\n",
    "    test_preds = model.predict(test_generator, steps=int(np.ceil(test_generator.samples/BATCH_SIZE)))\n",
    "    test_classes = np.argmax(test_preds, axis=1)\n",
    "    # Map predicted indices to class names\n",
    "    idx_to_class = {v: k for k, v in class_indices.items()}\n",
    "    test_class_names = [idx_to_class[idx] for idx in test_classes]\n",
    "    # Create and save submission file\n",
    "    submission_df = pd.DataFrame({\n",
    "        'image_id': test_df['image_id'],\n",
    "        'soil_type': test_class_names\n",
    "    })\n",
    "    submission_df.to_csv('enhanced_f1_95_submission.csv', index=False)\n",
    "    if f1_score_result >= 0.95:\n",
    "        print(f\"🎯 SUCCESS! F1 Score: {f1_score_result:.4f} ≥ 0.95\")\n",
    "    else:\n",
    "        print(f\"📊 Result: F1 Score: {f1_score_result:.4f} (Target: 0.95)\")\n",
    "    print(\"Enhanced submission saved as: enhanced_f1_95_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Submission\n",
    "We evaluate the model and generate predictions for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T19:45:27.838669Z",
     "iopub.status.busy": "2025-05-23T19:45:27.838278Z",
     "iopub.status.idle": "2025-05-23T19:58:45.041103Z",
     "shell.execute_reply": "2025-05-23T19:58:45.040325Z",
     "shell.execute_reply.started": "2025-05-23T19:45:27.838642Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Entry point for script execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    main_enhanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7477036,
     "sourceId": 11895167,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
